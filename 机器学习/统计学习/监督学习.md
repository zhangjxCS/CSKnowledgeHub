## Linear Regression线性回归

1.Linear Regression的基础假设是什么

线性性：因变量和自变量满足线性关系

独立性（误差项）：误差项之间相互独立

独立性（自变量）：各个自变量之间相互独立

正态性：误差项应呈正态分布

同方差性：误差项的方差为常数

2.what will happen when we have correlated variables, how to solve

删除变量：这个方法一般不推荐使用，因为删除变量会导致异方差增大

增加样本容量：增大样本量，减小多重共线性

变换模型：对数据求差分；计算相对指标；相关变量做线性组合，例如使用主成分分析等方法降维

逐步回归：常用方法，添加删除变量之后做可决系数、F检验和T检验来确定是否增加或者剔除变量，若果增加变量对这些指标的影响较小，也认为指标为多余的，如果增加指标引起R和F的变动且通不过T检验，说明存在共线性

岭回归：Ridge regression加入正则化项，减小自变量系数，减弱多重共线性

3.explain regression coefficient

回归分析只能得出相关关系，不能判断因果关系

当其他变量保持不变或控制其他变量不变时$X$每改变一个单位时因变量$Y$的平均变化量

4.what is the relationship between minimizing squared error and maximizing the likelihood

两种不同的求最优解的方法，最终计算出的结果是一致的

最小二乘法(OLS)基于频率派概率思想，求真值与估计值之间差值最小对应的参数

最大似然(MLE)基于贝叶斯思想，将参数看作概率分布，通过最大化似然函数来求参数

5.if the relationship between y and x is no linear, can linear regression solve that

可以添加高次变量，以及不同变量之间的组合，例如$x_1^2$ $x_1x_2$等交互变量作为自变量

## Logistic回归

Logistic Regression的loss是什么

logistic regression的损失函数为交叉熵损失函数
$$
J(\theta)=-[ylog(h(x))+(1-y)log(1-h(x))]
$$

## KNN

1.KNN算法

输入训练数据集，输出实例x所属的类y

(1)根据给定的距离算法，在训练集找出与x最邻近的k个点

(2)根据这些点多数点所属类别确定这个点的类别

KNN没有显式的学习过程，没有模型参数

2.KNN距离
$$
L_p(x_i,x_j)=(\sum|x_i-x_j|^p)^{1/p}
$$
P=1，称为曼哈顿距离

P=2，称为欧式距离

P=无穷时，是每个坐标距离的最大值

不同距离度量确定的最近邻点是不同的

 3.k值选择

k值较小，训练误差小，易发生过拟合。K=1时训练误差为零

k值较大，训练误差大，模型变简单，易发生欠拟合。K=N时类似于少数服从多数

在应用过程中，通过交叉验证(cross-validation)取验证误差最小时对应的k

4.KNN的实现：kd树

kd树是一种二叉树，表示对k维空间的一个划分

kd树的构造：

(1)构造根节点，使其对应于包含所有数据的k维空间超矩形区域，选择x1为坐标轴，以所有数据点x1值的中位数作为切分点，将超矩形区域分为两个子区域并生成根节点的左右子节点，分别对应小于和大于x1坐标的子区域，落在切分平面上的点保留在根节点

(2)对深度为j的节点，重复上述过程并生成j+1层的子节点直到两个子区域没有实例存在时停止

kd树的搜索：

(1)找到包含目标点x的叶节点（从根节点向下遍历）

(2)以此叶节点为当前最近点

(3)递归向上回退，先检查父节点是否更近，若是则更新最近点，若不是则检查另一子节点对应区域是否相交，若相交则在另一子节点对应区域查找，若不相交则回退至父节点

(4)按步骤3递归进行回退直到根节点

kd树搜索适用于训练实例树大于空间维度时的搜索，平均计算复杂度时O(logN)

## 决策树

1.概念

从逻辑角度，决策树是一些if else语句的组合；从几何角度，是根据某种准则划分特征空间；最终的目的是样本分类纯度高

决策树模型学习步骤：特征选择、决策树生成、决策树修剪

性质：决策树的路径互斥并且完备，每一个实例都被一条路径或者一条规则所覆盖，且仅被一条路径或规则所覆盖

学习目标：根据给定的训练数据构建模型，使其能够对实例进行正确分类，得到一个与训练数据矛盾较小，且具有较强泛化能力的模型 

2.特征选择：

选择准则：信息增益或信息增益比

随机变量X的熵：$H(X)=-\sum p_i \log p_i$

条件熵：$H(Y|X)=\sum p_iH(Y|X=x_i)$

信息增益：$g(D,A)=H(D)-H(D|A)$ 信息增益表示由于特征A而使得对数据集D的分类的不确定性减少程度

信息增益比：$g_R(D,A)=g(D,A)/H(D)$

3.决策树的生成

ID3算法：从根节点开始，对节点计算所有特征的信息增益，选择信息增益最大的特征作为节点特征，由该特征的不同取值建立子节点；再对子节点递归调用上述方法，构建决策树，直到所有节点信息增益小于某个阈值为止

C4.5算法：与ID3算法类似，选择信息增益比代替信息增益作为选择标准 

4.决策树的剪枝

在决策树的学习中将已生成的树进行简化的过程称为剪枝

决策树的损失函数如下，T为叶节点个数，N为第t个叶节点样本数，加L1正则化项
$$
C(T)=\sum N_tH_t(T)+\alpha |T|
$$
在具体操作过程中，考虑剪枝前后树的熵，若剪枝后熵小于剪枝前熵，则进行剪枝，这种算法可以用动态规划的方法实现

5.CART算法

分类树与回归树算法，CART假设决策树是二叉树，内部节点特征的取值为是和否，左分支为取值为是的分支，右分支为取值为否的分支，使用基尼系数来选择最优变量的最优拆分点

回归树：回归树的训练过程采用启发式的方法选择第i个变量和他的切分点s，通过最小化误差函数的方法选择最优切分变量和切分点，每个节点样本的均值作为测试样本的回归预测值

(1)选择最优切分变量与切分点
$$
min_{j,s}[min \sum_{R_1(j,s)}(y_i-c_1)^2+min\sum_{R_2(j,s)}(y_i-c_2)^2]
$$
遍历j，对固定的切分变量j，扫描切分点，使得上式达到最小，得到(j,s)

(2)用选定的(j,s)划分区域并决定输出值

(3)继续对子区域调用步骤1、2

(4)输入空间划分为M个区域，生成决策树

分类树：计算现有特征对数据集的基尼指数，在所有特征和切分点中选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点，每个节点样本的类别情况投票决定测试样本的类别

基尼指数：$Gini(p)=\sum p_k(1-p_k)$

基尼指数值越大，样本集合不确定性越大，与熵类似

(1)训练数据集为D，计算现有特征所有可能取值对数据集的基尼指数

(2)在所有可能特征和它们的切分点a中选择基尼指数最小的特征及其切分点作为最优特征与最优切分点，并生成两个子节点

(3)对子节点递归调用1、2

(4)生成决策树

CART剪枝：从生成算法产生的决策树底端开始不断剪枝，直到根节点，形成子树序列，然后通过交叉验证集对子树序列进行测试，选择最优子树

6.决策树的剪枝

可以通过预剪枝或者后剪枝的方法预防过拟合

预剪枝策略：

定义一个高度，当决策树达到该高度时就可以停止决策树的生长

达到某个结点的实例具有相同的特征向量，即使这些实例不属于同一类，也可以停止决策树的生长

定义一个阈值，当达到某个结点的实例个数小于该阈值时就可以停止决策树的生长

定义一个阈值，通过计算每次扩张对系统性能的增益，并比较增益值与该阈值的大小来决定是否停止决策树的生长

后剪枝策略：删除一些子树，然后用其叶子节点代替

REP方法是一种比较简单的后剪枝的方法，在该方法中，可用的数据被分成两个样例集合：一个训练集用来形成学习到的决策树，一个分离的验证集用来评估这个决策树在后续数据上的精度，确切地说是用来评估修剪这个决策树的影响

PEP,悲观错误剪枝,悲观错误剪枝法是根据剪枝前后的错误率来判定子树的修剪

7.决策树的正则化

通过设置最大深度，节点分裂阈值等方法来完成正则化

## 朴素贝叶斯

1.贝叶斯公式
$$
P(A|B)=\frac{P(A,B)}{P(B)}=\frac{P(B|A)P(A)}{P(B)}
$$
学习目标：联合概率分布$P(A,B)$

加入先验知识$P(A)$

最大似然估计与最大后验估计

MLE: 最大化似然函数

MAP: 最大化后验概率，估计得到参数的后验分布

伯努利分布：MAP先验分布选择beta分布，后验与先验分布形式一致

多项式分布：MAP先验分布选择dirichlet分布 

3.朴素贝叶斯分类器
$$
P(Y|X_1,X_2,...,X_N)=\frac{P(X_1,X_2,...,X_N|Y)P(Y)}{P(X_1,X_2,...,X_N)}
$$
左侧需要参数过多，为$2^{n+1}-1$个

朴素贝叶斯：假设feature是独立的

目标函数：$arg max P(Y=y_k)\pi P(X_i|Y=y_k)$

4.朴素贝叶斯与逻辑回归

朴素贝叶斯相比于逻辑回归收敛更快，需要样本更少

数据量足够大时，逻辑回归效果更好，考虑了不同变量之间的相关关系

## 支持向量机SVM

1.线性可分支持向量机

支持向量机（support vector machines, SVM）是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器。SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。

分离超平面：$wx+b=0$

分类决策函数：$f(x)=sign(wx+b)$

函数间隔：$y_i(wx_i+b)$

几何间隔：$\frac{y_i}{||w||}(wx_i+b)$

将间隔最大化问题转化为最优化问题：
$$
min \frac{1}{2}||w||^2
$$

$$
s.t. y_i(wx_i+b)-1\ge 0, i=1,2,...,N
$$

间隔最大化算法求出的分离超平面解是唯一的

支持向量：训练集中离分离超平面距离最近的样本点

间隔：正负支持向量与分离超平面之间距离之和，为$2/||w||$

对偶问题：构造拉格朗日函数求解
$$
L(w,b,\alpha)=\frac{1}{2}||w||^2-\sum_{i=1}^N \alpha_iy_i(wx_i+b)+\sum_{i=1}^N \alpha_i
$$
2.线性不可分支持向量机

相比于线性可分支持向量机，引入松弛变量$\epsilon_i$，允许存在错误分类的点。通过构造拉格朗日函数求解约束优化问题
$$
min \frac{1}{2}||w||^2
$$

$$
s.t. y_i(wx_i+b)-1\ge 1-\epsilon_i, i=1,2,...,N
$$

3.非线性支持向量机

对于输入空间中的非线性分类问题，可以通过非线性变换将它转化为某个维特征空间中的线性分类问题，在高维特征空间中学习线性支持向量机

在非线性支持向量机学习的对偶问题里，目标函数和分类决策函数都只涉及实例和实例之间的内积，所以不需要显式地指定非线性变换**，**而是用核函数替换当中的内积。

$$
K(x,z)=\phi (x)\sdot\phi(z)
$$
![img](https://i.loli.net/2021/08/02/oIqyPETceM7xbnD.jpg)

4.怎么把SVM的output按照概率输出

SVM分类器能输出（测试）样本和决策边界的距离，可以把这个距离当做一个置信度数值。可以用Logistic Regression把SVM输出的置信度数值校准为概率值

## Ensemble Learning集成学习

1.集成学习分类

一般来说集成学习可以分为三大类：

- 用于减少方差的bagging
- 用于减少偏差的boosting
- 用于提升预测结果的stacking

集成学习方法也可以归为如下两大类：

- 串行集成方法，这种方法串行地生成基础模型（如AdaBoost）。串行集成的基本动机是利用基础模型之间的依赖。通过给错分样本一个较大的权重来提升性能。
- 并行集成方法，这种方法并行地生成基础模型（如Random Forest）。并行集成的基本动机是利用基础模型的独立性，因为通过平均能够较大地降低误差。

2.Bagging

Bagging使用装袋采样来获取数据子集训练基础学习器。通常分类任务使用投票的方式集成，而回归任务通过平均的方式集成。Bagging中基学习器的数据是从所有数据中随机选择的，用于学习的特征也是随机选择一部分

在随机森林中，每个树模型都是装袋采样训练的。另外，特征也是随机选择的，最后对于训练好的树也是随机选择的，这种处理的结果是随机森林的偏差增加的很少，而由于弱相关树模型的平均，方差也得以降低，最终得到一个方差小，偏差也小的模型。

3.Boosting

Boosting指的是通过算法集合将弱学习器转换为强学习器。boosting的主要原则是训练一系列的弱学习器，所谓弱学习器是指仅比随机猜测好一点点的模型，例如较小的决策树，训练的方式是**利用加权的数据**。在训练的早期对于错分数据给予较大的权重。 

AdaBoost第一个分类器y1(x)是用相等的权重系数进行训练的。在随后的boosting中，错分的数据权重系数将会增加，正确分类的数据权重系数将会减小。

梯度树提升（Gradient Tree Boosting）是一个boosting算法在损失函数上的泛化。能够用于分类和回归问题。Gradient Boosting采用串行方式构建模型。

4.Stacking

Stacking是通过一个元分类器或者元回归器来整合多个分类模型或回归模型的集成学习技术。基础模型利用整个训练集做训练，元模型将基础模型的特征作为特征进行训练。

基础模型通常包含不同的学习算法，因此stacking通常是异质集成。

4.difference between bagging and boosting

样本选择上：

Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的

Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整

样例权重：

Bagging：使用均匀取样，每个样例的权重相等

Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大

预测函数：

Bagging：所有预测函数的权重相等

Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重

并行计算：

Bagging：各个预测函数可以并行生成

Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果

## 树模型Boosting

### GBDT

GBDT是通过Boosting的思路来对决策树模型进行提升。GBDT采用加法模型，每轮迭代会产生一个弱分类器， 每个分类器在上一轮分类器的残差基础上进行训练。 gbdt对弱分类器的要求一般是足够简单， 并且低方差高偏差，因此每棵分类回归树的深度较浅，最终的总分类器在每轮训练的弱分类器加权求和得到。

GBDT使用的弱分类器是CART回归树，但是可以用于分类问题也可以用于回归问题，分类与回归所对应的损失函数不同

• 分类问题：$Loss = \sum_i -y_i\log(p_i)-(1-y_i)\log(1-p_i)$

• 回归问题：$Loss = \sum_i (y_i - \hat y_i)^2$

### XGBoost

- 首先，对所有特征都按照特征的数值进行预排序。
- 其次，在遍历分割点的时候用O(data)的代价找到一个特征上的最好分割点。
- 最后，找到一个特征的分割点后，将数据分裂成左右子节点。
- 优点：

算法本身的优化：在算法的弱学习器模型选择上，对比GBDT只支持决策树，还可以直接很多其他的弱学习器。在算法的损失函数上，除了本身的损失，还加上了正则化部分。在算法的优化方式上，GBDT的损失函数只对误差部分做负梯度（一阶泰勒）展开，而**XGBoost损失函数对误差部分做二阶泰勒展开**，更加准确。算法本身的优化是我们后面讨论的重点。

算法运行效率的优化：对每个弱学习器，比如决策树建立的过程做并行选择，找到合适的子树分裂特征和特征值。在并行选择之前，先对所有的特征的值进行排序分组，方便前面说的并行选择。对分组的特征，选择合适的分组大小，使用CPU缓存进行读取加速。将各个分组保存到多个硬盘以提高IO速度。

算法健壮性的优化：对于缺失值的特征，通过枚举所有缺失值在当前节点是进入左子树还是右子树来决定缺失值的处理方式。算法本身加入了L1和L2正则化项，可以防止过拟合，泛化能力更强。

- 缺点：

首先，空间消耗大。这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如排序后的索引，为了后续快速的计算分割点），这里需要消耗训练数据两倍的内存。

其次，时间上也有较大的开销，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。

最后，对 cache 优化不友好。在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对 cache 进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的 cache miss。  

### LightGBM

**直方图算法**

基本思想是先把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。

优点：

内存消耗的降低，直方图算法不仅不需要额外存储预排序的结果，而且可以只保存特征离散化后的值，而这个值一般用 8 位整型存储就足够了，内存消耗可以降低为原来的1/8

计算上的代价也大幅降低，预排序算法每遍历一个特征值就需要计算一次分裂的增益，而直方图算法只需要计算k次（k可以认为是常数），时间复杂度从O(#data*#feature)优化到O(k*#features)

**带深度限制的 Leaf-wise 的叶子生长策略**

Leaf-wise 则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同 Level-wise 相比，在分裂次数相同的情况下，Leaf-wise 可以降低更多的误差，得到更好的精度。Leaf-wise 的缺点是可能会长出比较深的决策树，产生过拟合。因此 LightGBM 在 Leaf-wise 之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。  

**直方图加速**

一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到。通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的k个桶。利用这个方法，LightGBM 可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍

**LightGBM并行优化**

LightGBM 还具有支持高效并行的优点。LightGBM 原生支持并行学习，目前支持**特征并行**和**数据并行**的两种。

- 特征并行的主要思想是在不同机器在不同的特征集合上分别寻找最优的分割点，然后在机器间同步最优的分割点。
- 数据并行则是让不同的机器先在本地构造直方图，然后进行全局的合并，最后在合并的直方图上面寻找最优分割点。

在特征并行算法中，通过在本地保存全部数据避免对数据切分结果的通信；

在数据并行中使用分散规约 (Reduce scatter) 把直方图合并的任务分摊到不同的机器，降低通信和计算，并利用直方图做差，进一步减少了一半的通信量。基于投票的数据并行则进一步优化数据并行中的通信代价，使通信代价变成常数级别。在数据量很大的时候，使用投票并行可以得到非常好的加速效果。

## 树模型Bagging

### Random Forest

随机森林是采用Bagging的思想，构建多棵决策树，再通过多棵决策树模型平均的方法构建学习器。对于不同树的样本，通过bootstrap采样的方法得到，不同树采用的特征，也通过bootstrap采样得到

随机森林随机选择数据集和特征，特征数量小于总特征，多个分类器的平均结果就是最终预测的结果，而多个分类器的方差相似，去平均后方差为原来的1/n，所以随机森林可以减小方差，但是均值与单个模型的均值相同，所以不能减小偏差

GBDT和Random Forest区别：

• 随机森林采用的bagging思想，而GBDT采用的boosting思想

• 组成随机森林的树可以是分类树，也可以是回归树；而GBDT只能由回归树组成

• 组成随机森林的树可以并行生成；而GBDT只能是串行生成

• 对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来

• 随机森林对异常值不敏感；GBDT对异常值非常敏感

• 随机森林对训练集一视同仁；GBDT是基于权值的弱分类器的集成

• 随机森林是通过减少模型方差提高性能；GBDT是通过减少模型偏差提高性能